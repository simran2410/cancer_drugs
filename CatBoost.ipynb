import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import train_test_split, KFold, cross_val_score, GridSearchCV, RandomizedSearchCV
from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error, accuracy_score, precision_score, f1_score, recall_score
from sklearn.preprocessing import StandardScaler
from catboost import CatBoostRegressor
from math import sqrt
import warnings
from scipy import stats
warnings.filterwarnings('ignore')


np.random.seed(42)

def load_data():
    
    df = pd.read_csv('dataset.csv')
    
    
    feature_names = ['feature1', 'feature2', 'feature3']  
    X_df = df[feature_names]
    
    
    y_series = df['target']  
    
    return X_df, y_series

def binarize_outputs(actual, predicted, threshold=None):
    
    if threshold is None:
        threshold = np.median(actual)
    actual_binary = (actual > threshold).astype(int)
    predicted_binary = (predicted > threshold).astype(int)
    return actual_binary, predicted_binary, threshold

def main():
   
    X, y = load_data()
    
    
    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)
    
    
    scaler = StandardScaler()
    X_train_scaled = scaler.fit_transform(X_train)
    X_test_scaled = scaler.transform(X_test)
    
    
    param_distributions = {
        'iterations': [100, 200, 300, 500],
        'depth': [4, 5, 6, 7, 8],
        'learning_rate': [0.05, 0.1, 0.15, 0.2],
        'subsample': [0.8, 0.9, 1.0],
        'colsample_bylevel': [0.8, 0.9, 1.0],
        'min_data_in_leaf': [1, 3, 5],
        'reg_lambda': [0, 0.1, 1.0],
        'random_strength': [0, 1.0],
        'bagging_temperature': [0, 1.0],
        'border_count': [128, 254]
    }
    
    print("Starting hyperparameter optimization...")
    
    
    base_estimator = CatBoostRegressor(
        random_seed=42,
        verbose=False,
        allow_writing_files=False,
        thread_count=2,
        early_stopping_rounds=50,
        eval_metric='RMSE'
    )
    
    
    random_search = RandomizedSearchCV(
        estimator=base_estimator,
        param_distributions=param_distributions,
        n_iter=50,
        cv=5,
        scoring='neg_root_mean_squared_error',
        n_jobs=2,
        verbose=1,
        random_state=42,
        return_train_score=True
    )
    
    
    try:
        random_search.fit(X_train_scaled, y_train)
        best_model = random_search.best_estimator_
        print("Hyperparameter optimization completed successfully!")
        print(f"Best CV score: {random_search.best_score_:.4f}")
    except Exception as e:
        print(f"Error in hyperparameter optimization: {e}")
        
        best_model = CatBoostRegressor(
            iterations=300,
            depth=6,
            learning_rate=0.1,
            random_seed=42,
            verbose=False,
            allow_writing_files=False
        )
        best_model.fit(X_train_scaled, y_train)
    
    
    y_pred = best_model.predict(X_test_scaled)
    y_train_pred = best_model.predict(X_train_scaled)
    
    
    y_train_binary, y_train_pred_binary, train_threshold = binarize_outputs(y_train, y_train_pred)
    y_test_binary, y_test_pred_binary, test_threshold = binarize_outputs(y_test, y_pred)
    
    
    errors = y_test.values - y_pred
    relative_errors = np.abs((y_test.values - y_pred) / y_test.values * 100)
    
    
    rmse = sqrt(mean_squared_error(y_test, y_pred))
    r2 = r2_score(y_test, y_pred)
    mae = mean_absolute_error(y_test, y_pred)
    
    train_rmse = sqrt(mean_squared_error(y_train, y_train_pred))
    train_r2 = r2_score(y_train, y_train_pred)
    train_mae = mean_absolute_error(y_train, y_train_pred)
    
    
    try:
        train_accuracy = accuracy_score(y_train_binary, y_train_pred_binary)
        train_precision = precision_score(y_train_binary, y_train_pred_binary, zero_division=0)
        train_recall = recall_score(y_train_binary, y_train_pred_binary, zero_division=0)
        train_f1 = f1_score(y_train_binary, y_train_pred_binary, zero_division=0)
        
        test_accuracy = accuracy_score(y_test_binary, y_test_pred_binary)
        test_precision = precision_score(y_test_binary, y_test_pred_binary, zero_division=0)
        test_recall = recall_score(y_test_binary, y_test_pred_binary, zero_division=0)
        test_f1 = f1_score(y_test_binary, y_test_pred_binary, zero_division=0)
    except Exception as e:
        print(f"Error calculating classification metrics: {e}")
        train_accuracy = train_precision = train_recall = train_f1 = 0
        test_accuracy = test_precision = test_recall = test_f1 = 0
    
    
    kf = KFold(n_splits=5, shuffle=True, random_state=42)
    try:
        cv_scores_rmse = -cross_val_score(best_model, X_train_scaled, y_train,
                                         scoring='neg_root_mean_squared_error', cv=kf, n_jobs=1)
        cv_scores_r2 = cross_val_score(best_model, X_train_scaled, y_train,
                                      scoring='r2', cv=kf, n_jobs=1)
        cv_scores_mae = -cross_val_score(best_model, X_train_scaled, y_train,
                                        scoring='neg_mean_absolute_error', cv=kf, n_jobs=1)
    except Exception as e:
        print(f"Error in cross-validation: {e}")
        cv_scores_rmse = cv_scores_r2 = cv_scores_mae = np.array([0])
    
    
    print("\n======= Performance Metrics =======")
    
    print("\n== Regression Metrics ==")
    print(f"Training RMSE: {train_rmse:.4f}")
    print(f"Testing RMSE: {rmse:.4f}")
    print(f"Training R²: {train_r2:.4f}")
    print(f"Testing R²: {r2:.4f}")
    print(f"Training MAE: {train_mae:.4f}")
    print(f"Testing MAE: {mae:.4f}")
    
    print("\n== Classification Metrics (Binarized) ==")
    print(f"Binarization threshold: {test_threshold:.4f}")
    print(f"Training Accuracy: {train_accuracy:.4f}")
    print(f"Testing Accuracy: {test_accuracy:.4f}")
    print(f"Training Precision: {train_precision:.4f}")
    print(f"Testing Precision: {test_precision:.4f}")
    print(f"Training Recall: {train_recall:.4f}")
    print(f"Testing Recall: {test_recall:.4f}")
    print(f"Training F1 Score: {train_f1:.4f}")
    print(f"Testing F1 Score: {test_f1:.4f}")
    
    print("\n== 5-Fold Cross-Validation Results ==")
    print(f"CV RMSE: {cv_scores_rmse.mean():.4f} (±{cv_scores_rmse.std():.4f})")
    print(f"CV R²: {cv_scores_r2.mean():.4f} (±{cv_scores_r2.std():.4f})")
    print(f"CV MAE: {cv_scores_mae.mean():.4f} (±{cv_scores_mae.std():.4f})")
    
    
    create_plots(y_test, y_pred, errors)
    
    print("\n" + "="*50)
    print("Model training and evaluation completed successfully!")
    print("="*50)

def create_plots(y_test, y_pred, errors):
    
    try:
       
        plt.figure(figsize=(8, 6))
        plt.scatter(y_test, y_pred,
                   alpha=0.6,
                   color='red',
                   marker='o',
                   s=50,
                   edgecolor='black',
                   linewidth=0.3)
        
        plt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()],
                color='black',
                linestyle='-',
                linewidth=2)
        
        plt.title("Actual vs Predicted Values", fontsize=14)
        plt.xlabel("Actual", fontsize=12)
        plt.ylabel("Predicted", fontsize=12)
        plt.grid(True, alpha=0.3)
        plt.tight_layout()
        plt.show()
        
        
        plt.figure(figsize=(8, 6))
        plt.scatter(y_pred, errors,
                   alpha=0.6,
                   color='#3498db',
                   marker='o',
                   s=50,
                   edgecolor='white',
                   linewidth=0.3)
        
        plt.axhline(y=0,
                   color='#e74c3c',
                   linestyle='--',
                   linewidth=2)
        
        plt.title("Residual Plot", fontsize=14)
        plt.xlabel("Predicted", fontsize=12)
        plt.ylabel("Residuals", fontsize=12)
        plt.grid(True, alpha=0.3)
        plt.tight_layout()
        plt.show()
        
    except Exception as e:
        print(f"Error in plotting: {e}")

if __name__ == "__main__":
    main()